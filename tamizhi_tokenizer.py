# tamizhi_tokenizer.py

import re
from typing import List, Optional

# ğŸ”¸ à®’à®µà¯à®µà¯Šà®°à¯ Token à®‡à®©à¯ à®µà®•à¯ˆà®¯à¯ˆ à®µà®°à¯ˆà®¯à®±à¯à®•à¯à®•à®¿à®©à¯à®±à¯‹à®®à¯
class TokenType:
    IDENTIFIER = "IDENTIFIER"
    NUMBER = "NUMBER"
    STRING = "STRING"
    KEYWORD = "KEYWORD"
    SYMBOL = "SYMBOL"
    EOF = "EOF"

# ğŸ”¸ Tamizhi à®®à¯Šà®´à®¿à®•à¯à®•à®¾à®© à®®à¯à®•à¯à®•à®¿à®¯ à®•à¯à®±à®¿à®¯à¯€à®Ÿà¯à®Ÿà¯ à®µà®¾à®°à¯à®¤à¯à®¤à¯ˆà®•à®³à¯
TAMIZHI_KEYWORDS = {
    "à®†à®•": "let",         # let a = 5
    "à®¨à®¿à®°à®²à¯à®ªà®¾à®•à®®à¯": "function",
    "à®ªà®¿à®©à¯à®•à¯Šà®Ÿà¯": "return",
    "à®†à®©à®¾à®²à¯": "else",
    "à®†à®©à®¾à®²à¯_à®‡à®²à¯à®²à¯ˆ": "elif",
    "à®†à®•à®¿à®²à¯": "if",
    "à®µà®°à¯ˆ": "while",
    "à®ªà®¤à®¿à®ªà¯à®ªà®¿": "print",
    "à®‰à®³à¯à®³à¯€à®Ÿà¯": "input",
    "à®šà®°à®¿": "true",
    "à®¤à®µà®±à¯": "false",
    "à®‡à®²à¯à®²à¯ˆ": "null"
}

# ğŸ”¸ Token à®µà®•à¯à®ªà¯à®ªà¯à®•à®³à¯
class Token:
    def __init__(self, type_: str, value: str, line: int, column: int):
        self.type = type_
        self.value = value
        self.line = line
        self.column = column

    def __repr__(self):
        return f"Token({self.type}, {self.value}, Line: {self.line}, Col: {self.column})"

# ğŸ”¸ à®‰à®°à¯ˆà®¯à¯ˆ Tokenà®•à®³à®¾à®• à®®à®¾à®±à¯à®±à¯à®®à¯ à®¤à¯Šà®•à¯à®ªà¯à®ªà®¾à®©à¯
class Tokenizer:
    def __init__(self, source: str):
        self.source = source
        self.position = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []

    def tokenize(self) -> List[Token]:
        while not self._is_at_end():
            self._skip_whitespace()
            start_pos = self.position
            char = self._peek()

            if char.isalpha() or self._is_tamil_letter(char):
                self._identifier_or_keyword()
            elif char.isdigit():
                self._number()
            elif char == '"':
                self._string()
            elif char in "+-*/=(){}<>!,:;":
                self._symbol()
            else:
                self._error(f"à®…à®±à®¿à®¯à®ªà¯à®ªà®Ÿà®¾à®¤ à®à®´à¯à®¤à¯à®¤à¯: '{char}'")
                self._advance()
        
        self.tokens.append(Token(TokenType.EOF, "", self.line, self.column))
        return self.tokens

    def _is_tamil_letter(self, char: str) -> bool:
        return '\u0B80' <= char <= '\u0BFF'

    def _identifier_or_keyword(self):
        start_line, start_col = self.line, self.column
        start = self.position
        while not self._is_at_end() and (self._peek().isalnum() or self._is_tamil_letter(self._peek()) or self._peek() == "_"):
            self._advance()
        value = self.source[start:self.position]
        token_type = TokenType.KEYWORD if value in TAMIZHI_KEYWORDS else TokenType.IDENTIFIER
        self.tokens.append(Token(token_type, value, start_line, start_col))

    def _number(self):
        start_line, start_col = self.line, self.column
        start = self.position
        while not self._is_at_end() and self._peek().isdigit():
            self._advance()
        self.tokens.append(Token(TokenType.NUMBER, self.source[start:self.position], start_line, start_col))

    def _string(self):
        start_line, start_col = self.line, self.column
        self._advance()  # skip opening "
        start = self.position
        while not self._is_at_end() and self._peek() != '"':
            if self._peek() == "\n":
                self.line += 1
                self.column = 1
            self._advance()
        if self._is_at_end():
            self._error("à®®à¯‚à®Ÿà®¿à®¯ à®¤à¯Šà®Ÿà®•à¯à®•à®®à¯ à®‡à®²à¯à®²à®¾à®¤ à®à®´à¯à®¤à¯à®¤à¯ à®µà®°à®¿ (Unterminated string)")
            return
        value = self.source[start:self.position]
        self._advance()  # skip closing "
        self.tokens.append(Token(TokenType.STRING, value, start_line, start_col))

    def _symbol(self):
        start_line, start_col = self.line, self.column
        value = self._advance()
        self.tokens.append(Token(TokenType.SYMBOL, value, start_line, start_col))

    def _skip_whitespace(self):
        while not self._is_at_end() and self._peek().isspace():
            if self._peek() == "\n":
                self.line += 1
                self.column = 1
            else:
                self.column += 1
            self.position += 1

    def _peek(self) -> str:
        return self.source[self.position] if not self._is_at_end() else '\0'

    def _advance(self) -> str:
        char = self.source[self.position]
        self.position += 1
        self.column += 1
        return char

    def _is_at_end(self) -> bool:
        return self.position >= len(self.source)

    def _error(self, message: str):
        print(f"[à®¤à¯Šà®•à¯à®ªà¯à®ªà¯ à®ªà®¿à®´à¯ˆ] à®µà®°à®¿ {self.line}, à®¨à¯†à®Ÿà¯à®µà®°à®¿ {self.column}: {message}")

# ğŸ”¸ à®šà¯‹à®¤à®©à¯ˆà®•à¯à®•à¯ à®‰à®¤à®¾à®°à®£à®®à¯
if __name__ == "__main__":
    code = '''
    à®†à®• x = 10
    à®ªà®¤à®¿à®ªà¯à®ªà®¿ "à®µà®£à®•à¯à®•à®®à¯"
    à®†à®•à®¿à®²à¯ x > 5:
        à®ªà®¤à®¿à®ªà¯à®ªà®¿ "x à®ªà¯†à®°à®¿à®¯à®¤à¯"
    '''
    tokenizer = Tokenizer(code)
    tokens = tokenizer.tokenize()
    for tok in tokens:
        print(tok)
